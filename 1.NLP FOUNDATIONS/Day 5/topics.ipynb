{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70885fd3",
   "metadata": {},
   "source": [
    "UNIGRAM, BIGRAM, TRIGRAM\n",
    "\n",
    "- UNIGRAM ignores context. it only cares how often each individual word appears.\n",
    "\n",
    "Eg: \"I love NLP\"\n",
    "\n",
    "=> 'I', 'love', 'NLP'\n",
    "\n",
    "unigram model probability:\n",
    "\n",
    "        P(w) = Count(w) / Total number of words\n",
    "\n",
    "like if \"love\" appears 10 times out of 100 words, then:\n",
    "        P(love) = 10/100 = 0.10\n",
    "\n",
    "Unigram treat every word independently - linke random words thrown together.\n",
    "\n",
    "\n",
    "- BIGRAM (2 words together) considers the previous one word to predict the next word\n",
    "\n",
    "EG: \n",
    "\"I love\", \"love NLP\"\n",
    "\n",
    "formula:\n",
    "        P(w2|w1) = count(w1,w2) / count(w1)\n",
    "\n",
    "like if \"I love\" appears 5 times, and 'I' appears 20 times:\n",
    "        P(love|I) = 5/20 = 0.25\n",
    "\n",
    "Bigram captures context better because it knows what word came before.\n",
    "\n",
    "\n",
    "- TRIGRAM (3 words together) looks at the previous two words to predict the next word\n",
    "\n",
    "EG: \n",
    "\"I love NLP\"\n",
    "\n",
    "formula:\n",
    "        P(w3​∣w1​,w2​) = Count(w1​,w2​,w3​)​ / Count(w1​,w2​)\n",
    "\n",
    "If 'I love NLP' appears 3 times and \"I love\" appears 5 times:\n",
    "        P(NLP|I, love) = 3/5 = 0.6\n",
    "\n",
    "Trigram understands short phrase structure better than unigram/bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177efc7",
   "metadata": {},
   "source": [
    "PROBABILITY OF SENTENCES: \n",
    "\n",
    "-- A language model assign a probability to a sentance based on how likey the words appear together in natural language.\n",
    "\n",
    "we calculate it using unigrams, bigrams, or trigrams, and we multiply the conditional probability of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad7cfd3",
   "metadata": {},
   "source": [
    "LAPLACE SMOOTHING (Add-1 Smoothing)\n",
    "\n",
    "Sometimes a bigram or trigram never appeared in training. that gives probability = zero, which kills the entire sentence probability\n",
    "\n",
    "Add-1 smoothing fixes this by adding 1 to every count so nothing is zero.\n",
    "\n",
    "suppose  the word \"eat pizza\"\n",
    "\n",
    "but the probability seems zero, This means the whole sentence probability becomes zero, even if it's a valid phrase.\n",
    "\n",
    "so at default we add one to the probability so the probability is small, but not impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a98653",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
