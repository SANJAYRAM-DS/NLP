{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f5c1ba5",
   "metadata": {},
   "source": [
    "TOKENIZATION is a process of splitting text into meaningful units called tokens(words, sentences, symbols, emojis, etc.)\n",
    "\n",
    "computers cannot process long sentences directly - they understand text only when broken into pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ac3f2",
   "metadata": {},
   "source": [
    "TYPES:\n",
    "\n",
    "Word Tokenization - Break text into words\n",
    "\n",
    "Sentence Tokenizaation - Break text into sentences\n",
    "\n",
    "Character Tokenization -> Breakdown into characters\n",
    "\n",
    "Subword Tokenization -> split rare/complex words into smaller parts -> \"unhappiness\" -> ['un', 'happiness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016a81b",
   "metadata": {},
   "source": [
    "NORMALIZATION\n",
    "\n",
    "Normalization means making text uniform and structures so that similar words become comparable\n",
    "\n",
    "Running -> runs, run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24ccc9",
   "metadata": {},
   "source": [
    "TECHNIQUES:\n",
    "\n",
    "LowerCasing\n",
    "\n",
    "Removing Punctuation\n",
    "\n",
    "Removing Stopwords\n",
    "\n",
    "Stemming\n",
    "\n",
    "Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf777714",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
