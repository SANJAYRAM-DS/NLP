{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc03821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there!', \"I'm learning NLP.\", 'It is fun.']\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello there! I'm learning NLP. It is fun.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf2e66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', '!', 'I', \"'m\", 'learning', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello there! I'm learning NLP.\"\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb35e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning is fun!\n"
     ]
    }
   ],
   "source": [
    "#Normalization\n",
    "#LOWERCASING\n",
    "text = \"Machine Learning Is Fun!\"\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e05b18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello NLP It is Fun\n"
     ]
    }
   ],
   "source": [
    "#Removing Punctuation\n",
    "import string\n",
    "\n",
    "text = \"Hello, NLP!!! It is Fun!\"\n",
    "clean = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c082b5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP', 'interesting', 'field', 'artificial', 'intelligence']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#StopWord Removal\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "text = \"NLP is an interesting field in artificial intelligence\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "filtered = [w for w in tokens if w.lower() not in stop_words]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b023f794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "#Stemming vs Lemmatizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "print(stemmer.stem(\"running\"))\n",
    "print(lemm.lemmatize(\"running\", \"v\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9635061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', '@sanjay', 'üòé', 'Learning', '#NLP', 'and', '#MachineLearning']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize hashtags, usernames, emojis, and words separately\n",
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    pattern = r\"(@\\w+|#\\w+|[\\U0001F600-\\U0001F64F]|[\\w']+)\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "text = \"Hey @sanjay üòéüî• Learning #NLP and #MachineLearning!\"\n",
    "print(custom_tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b770c9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', '‚ù§Ô∏è', 'learning', 'nlp', 'amazing', 'exciting', 'üòç']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    tokens = [w for w in tokens if w not in stop]\n",
    "    \n",
    "    # lemmatize\n",
    "    lemm = WordNetLemmatizer()\n",
    "    tokens = [lemm.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "normalize(\"I LOVE ‚ù§Ô∏è learning NLP!!! It's amazing and exciting üòç\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
