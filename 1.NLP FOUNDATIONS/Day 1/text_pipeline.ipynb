{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56f738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\srmpc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\srmpc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"punkt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d26ee7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial', 'intelligence', '(', 'AI', ')', 'is', 'a', 'broad', 'field', 'of', 'computer', 'science', 'focused', 'on', 'building', 'machines', 'capable', 'of', 'performing', 'tasks', 'that', 'typically', 'require', 'human', 'intelligence', ',', 'such', 'as', 'learning', ',', 'reasoning', ',', 'problem-solving', ',', 'and', 'understanding', 'language', '.', 'Key', 'Concepts', 'Learning', ':', 'AI', 'systems', 'learn', 'from', 'vast', 'amounts', 'of', 'data', ',', 'recognizing', 'patterns', 'and', 'trends', 'to', 'improve', 'their', 'performance', 'over', 'time', '.', 'Reasoning', 'and', 'Problem-Solving', ':', 'AI', 'enables', 'machines', 'to', 'analyze', 'information', ',', 'make', 'autonomous', 'decisions', ',', 'and', 'solve', 'complex', 'problems', 'without', 'constant', 'human', 'intervention', '.', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ':', 'A', 'key', 'subfield', 'of', 'AI', 'that', 'allows', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'respond', 'to', 'human', 'language', '(', 'e.g.', ',', 'virtual', 'assistants', ',', 'chatbots', ')', '.', 'Machine', 'Learning', '(', 'ML', ')', 'and', 'Deep', 'Learning', '(', 'DL', ')', ':', 'These', 'are', 'core', 'technologies', 'within', 'AI', '.', 'ML', 'involves', 'training', 'computers', 'with', 'data', ',', 'while', 'DL', 'uses', 'neural', 'networks', 'with', 'multiple', 'layers', 'to', 'process', 'even', 'more', 'complex', 'data', ',', 'such', 'as', 'images', 'and', 'video', '.', 'Generative', 'AI', ':', 'A', 'subset', 'of', 'AI', 'capable', 'of', 'creating', 'new', 'content', ',', 'such', 'as', 'images', ',', 'text', ',', 'and', 'code', ',', 'based', 'on', 'patterns', 'learned', 'from', 'existing', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Artificial intelligence (AI) is a broad field of computer science focused on building machines capable of performing tasks that typically require human intelligence, such as learning, reasoning, problem-solving, and understanding language. Key Concepts Learning: AI systems learn from vast amounts of data, recognizing patterns and trends to improve their performance over time. Reasoning and Problem-Solving: AI enables machines to analyze information, make autonomous decisions, and solve complex problems without constant human intervention. Natural Language Processing (NLP): A key subfield of AI that allows computers to understand, interpret, and respond to human language (e.g., virtual assistants, chatbots). Machine Learning (ML) and Deep Learning (DL): These are core technologies within AI. ML involves training computers with data, while DL uses neural networks with multiple layers to process even more complex data, such as images and video. Generative AI: A subset of AI capable of creating new content, such as images, text, and code, based on patterns learned from existing data.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a328af97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial', 'intelligence', '(', 'AI', ')', 'broad', 'field', 'computer', 'science', 'focused', 'building', 'machines', 'capable', 'performing', 'tasks', 'typically', 'require', 'human', 'intelligence', ',', 'learning', ',', 'reasoning', ',', 'problem-solving', ',', 'understanding', 'language', '.', 'Key', 'Concepts', 'Learning', ':', 'AI', 'systems', 'learn', 'vast', 'amounts', 'data', ',', 'recognizing', 'patterns', 'trends', 'improve', 'performance', 'time', '.', 'Reasoning', 'Problem-Solving', ':', 'AI', 'enables', 'machines', 'analyze', 'information', ',', 'make', 'autonomous', 'decisions', ',', 'solve', 'complex', 'problems', 'without', 'constant', 'human', 'intervention', '.', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ':', 'key', 'subfield', 'AI', 'allows', 'computers', 'understand', ',', 'interpret', ',', 'respond', 'human', 'language', '(', 'e.g.', ',', 'virtual', 'assistants', ',', 'chatbots', ')', '.', 'Machine', 'Learning', '(', 'ML', ')', 'Deep', 'Learning', '(', 'DL', ')', ':', 'core', 'technologies', 'within', 'AI', '.', 'ML', 'involves', 'training', 'computers', 'data', ',', 'DL', 'uses', 'neural', 'networks', 'multiple', 'layers', 'process', 'even', 'complex', 'data', ',', 'images', 'video', '.', 'Generative', 'AI', ':', 'subset', 'AI', 'capable', 'creating', 'new', 'content', ',', 'images', ',', 'text', ',', 'code', ',', 'based', 'patterns', 'learned', 'existing', 'data', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\srmpc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Stop word removal\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered = [w for w in tokens if w.lower() not in stop_words]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c23faba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artifici', 'intellig', '(', 'ai', ')', 'broad', 'field', 'comput', 'scienc', 'focus', 'build', 'machin', 'capabl', 'perform', 'task', 'typic', 'requir', 'human', 'intellig', ',', 'learn', ',', 'reason', ',', 'problem-solv', ',', 'understand', 'languag', '.', 'key', 'concept', 'learn', ':', 'ai', 'system', 'learn', 'vast', 'amount', 'data', ',', 'recogn', 'pattern', 'trend', 'improv', 'perform', 'time', '.', 'reason', 'problem-solv', ':', 'ai', 'enabl', 'machin', 'analyz', 'inform', ',', 'make', 'autonom', 'decis', ',', 'solv', 'complex', 'problem', 'without', 'constant', 'human', 'intervent', '.', 'natur', 'languag', 'process', '(', 'nlp', ')', ':', 'key', 'subfield', 'ai', 'allow', 'comput', 'understand', ',', 'interpret', ',', 'respond', 'human', 'languag', '(', 'e.g.', ',', 'virtual', 'assist', ',', 'chatbot', ')', '.', 'machin', 'learn', '(', 'ml', ')', 'deep', 'learn', '(', 'dl', ')', ':', 'core', 'technolog', 'within', 'ai', '.', 'ml', 'involv', 'train', 'comput', 'data', ',', 'dl', 'use', 'neural', 'network', 'multipl', 'layer', 'process', 'even', 'complex', 'data', ',', 'imag', 'video', '.', 'gener', 'ai', ':', 'subset', 'ai', 'capabl', 'creat', 'new', 'content', ',', 'imag', ',', 'text', ',', 'code', ',', 'base', 'pattern', 'learn', 'exist', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print([stemmer.stem(word) for word in filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d217883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\srmpc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial', 'intelligence', '(', 'AI', ')', 'broad', 'field', 'computer', 'science', 'focus', 'build', 'machine', 'capable', 'perform', 'task', 'typically', 'require', 'human', 'intelligence', ',', 'learn', ',', 'reason', ',', 'problem-solving', ',', 'understand', 'language', '.', 'Key', 'Concepts', 'Learning', ':', 'AI', 'systems', 'learn', 'vast', 'amount', 'data', ',', 'recognize', 'pattern', 'trend', 'improve', 'performance', 'time', '.', 'Reasoning', 'Problem-Solving', ':', 'AI', 'enable', 'machine', 'analyze', 'information', ',', 'make', 'autonomous', 'decisions', ',', 'solve', 'complex', 'problems', 'without', 'constant', 'human', 'intervention', '.', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', ':', 'key', 'subfield', 'AI', 'allow', 'computers', 'understand', ',', 'interpret', ',', 'respond', 'human', 'language', '(', 'e.g.', ',', 'virtual', 'assistants', ',', 'chatbots', ')', '.', 'Machine', 'Learning', '(', 'ML', ')', 'Deep', 'Learning', '(', 'DL', ')', ':', 'core', 'technologies', 'within', 'AI', '.', 'ML', 'involve', 'train', 'computers', 'data', ',', 'DL', 'use', 'neural', 'network', 'multiple', 'layer', 'process', 'even', 'complex', 'data', ',', 'image', 'video', '.', 'Generative', 'AI', ':', 'subset', 'AI', 'capable', 'create', 'new', 'content', ',', 'image', ',', 'text', ',', 'code', ',', 'base', 'pattern', 'learn', 'exist', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print([lemmatizer.lemmatize(word, pos='v') for word in filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "187d645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm loving it!\n"
     ]
    }
   ],
   "source": [
    "# Text basic normalozation\n",
    "from ftfy import fix_text\n",
    "s = \"Iâ€™m loving it!\"\n",
    "print(fix_text(s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
